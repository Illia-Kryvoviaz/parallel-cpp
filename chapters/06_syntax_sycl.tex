\chapter{The Syntax in SYCL}

SYCL is an open specification for the design of code targeting either CPUs or GPGPUs. 
The current version is SYCL 2020, released in 2021. 
If you want to learn more about SYCL, see 
\url{https://github.com/codeplaysoftware/syclacademy} for an extensive tutorial.


\section{More concepts}

To make as efficient use of GPGPUs as possible, SYCL utilizes a number of concepts that 
we have not seen earlier.

\subsection{Device selector}

Selectors are used to pick device to run on.
In header \cpp{<device_selector.h>}, there is an abstract class \cpp{device_selector} with numerous implementations
such as \cpp{gpu_selector}, \cpp{host_selector}, \cpp{opencl_selector}, and \cpp{default_selector}.

One can also implement its own subclasses that specify to the runtime how to perform device selection.
For example, it may query the amount of memory on the GPGPU and if it is sufficient, 
use GPGPU. If it were not sufficient, it could use CPU as a fallback. 
There, one overrides \cpp{int operator()(const sycl::device& dev) const override}
and returns an integer for the priority. The higher integer, the higher priority. 

\subsection{Queues}

A queue, \cpp{queue}, is an abstraction of a device,
through which we orchestrate work on the device.
In a constructor of a queue, we pass the device, which cannot be changed later,
but one can create further queues for the same device.
A key method is \cpp{queue.submit},
which passes a ``command group'' for asynchronous execution. 

A command group is, essentially, composed of a set of requirements and actual commands executing a kernel. 
The kernel then receives a ``command group handler'' from the SYCL as its argument,
and uses it to access the API. 
A command group submission to the group is atomic.  

Asynchronous execution also means an undefined order of execution, 
unless we use \cpp{wait} or suggest the dependencies between the ``actions'' in the form of a task graph.
We can also declare the queue to be in-order, similar to sorted in OpenMP:
\cpp{queue q{property::queue::in_order()};}.

%To specify the task graph, one can use events, e.g., 

 \subsection{Work items, Work groups, and Kernels}

Within an action submitted to the queue, we execute kernels.
 Kernels are callables 
 \begin{itemize}
 \item receiving an index to the run of the kernel as \cpp{auto idx} or \cpp{id<1> idx} or similar.
 \item returning nothing; with \cpp{void} return type 
 \item which cannot allocate memory dynamically
 \itme which cannot use certain other features (e.g., RTTI). 
\end{itemize}
Within the \cpp{single_task function} method of the ``command group handler'' API, 
we pass a C++ function object as a parameter and have it executed once. 
Kernel can also be a class that overloads operator (), e.g. \cpp{void operator()(id<1> idx)}.

Most often, we want the kernel executed many times, in a data-parallel fashion. 
In the so-called nd-ranges, we partition the index-set of a data set hierarchically first into into global ranges, and then into local ranges. 
The local range corresponds to a work-group and each element corresponds to a work item (= single run of a kernel). 
Typically, all work items within a work group are executed in lock step (i.e., the same hardware instruction in all work items at the same time).
The work-group local memory can often be accessed very efficiently, via \cpp{local_accessor }, and can be used to coordinate multiple work items (= single runs) within a work group.
%On the other hand, work-groups are independent of each other there is no way to synchronize write access between two work items in two distinct work groups in a single kernel. 

To summarize, each work item can access:
\begin{itemize}
\item private memory
\item work-group local memory
\item global memory accessible to all work items within an nd-range, but whose access can be very expensive, as it involves copying data across PCIe bus
\item constant memory, which is a part of the global memory, but which can be very cheap to access. 
\end{itemize}

\subsection{Asynchronous errors}

The SYCL implementation may throw ``synchronous errors'' (one at a time).
In contrast, asynchronous errors are produced by a command group or a kernel (with many kernels running at any point). By default, asynchronous errors are not propagated to the host. One can, however, defined and error handler and pass it to a queue \cpp{queue q(default_selector{}, exception_handler);}.
The error handler receives an \cpp{exception_list}, wherein one can iterate over \cpp{std::exception_ptr}.

See \url{https://www.codingame.com/playgrounds/48226/introduction-to-sycl/error-handling} for a great tutorial with code that is editable, compilable, and runnable online. Let us simplify their main example somewhat:

\raggedbottom
\begin{codebox}[]{\href{https://godbolt.org/z/}{\ExternalLink}}
\footnotesize Error handling in SYCL.
\tcblower
\cppfile{code_examples/sycl/exception1.cpp}
\end{codebox}
  
\subsection{Unified shared memory}
 
At the cost of some latency, one can use a unified shared memory across both the host and the device, wherein oneuses the same pointer on both the host and the device. This requires:
 
\cpp{
template <typename T>
T* malloc_shared(size_t count,
                 const queue& syclQueue,
                 const property_list &propList = {})
}

and the corresponding 

\cpp{void free(void* ptr, sycl::queue& syclQueue)}

See the example below:
\raggedbottom
\begin{codebox}[]{\href{https://godbolt.org/z/}{\ExternalLink}}
\footnotesize An example of the use of unified shared memory.
\tcblower
\cppfile{code_examples/sycl/buffer2.h}
\end{codebox}
 
\subsection{Buffers and accessors}

A buffer is a constrained view of a 1-, 2-, or 3-dimensional array.
The constraints specify how it can be accessed on the host, the device or both. 
A buffer is constructed with a pre-allocated, trivially copyable C++ objects (e.g., STL container). 
Within the contract for the use of the buffer, 
one promises not to amend the memory used to initialise the buffer during the lifetime of the buffer. 
Buffer promises to update the memory in the host upon destruction, in RAII spirit. 

In the case of one-dimensional arrays, one can call the constructor with an interator:
\cpp{template <typename InputIterator> buffer(InputIterator first, InputIterator last, const property_list &propList={});}

Once in a kernel, an \cpp{accessor} specifies constraints on the use of a buffer therein. 
Two key choices are:
\begin{itemize}
\item access mode: \cpp{read}, \cpp{write}, and \cpp{read_write}, where write access mode also implicitly defines dependencies between tasks
\item access target: \cpp{global_memory} suggests that the data resides in the global memory space of the device. Other options are device specific. \cpp{no_init} suggests that the initial data can be discarded (not moved to the device). 
\end{itemize}

See the example below:
\raggedbottom
\begin{codebox}[]{\href{https://godbolt.org/z/}{\ExternalLink}}
\footnotesize An example of the use of buffers and accessors.
\tcblower
\cppfile{code_examples/sycl/buffer1.h}
\end{codebox}

\subsection{Barrier}

Depending on the details of the use of a barrier, one may wish to use \cpp{sycl::queue::wait()} and \cpp{sycl::queue::wait_and_throw()}, or \cpp{item::barrier(access::fence_space)} within a kernel.


\subsection{More complex examples}

First, let us consider a complete, working example:

\raggedbottom
\begin{codebox}[]{\href{https://godbolt.org/z/}{\ExternalLink}}
\footnotesize An example of vector addition in SYCL.
\tcblower
\cppfile{code_examples/sycl/buffer3.cpp}
\end{codebox}

Next, let us consider an example that uses work groups and local memory in an attempt to utilize more of the performance available in the GPGPU. 

\raggedbottom
\begin{codebox}[]{\href{https://godbolt.org/z/}{\ExternalLink}}
\footnotesize An example of the use of work groups and local memory.
\tcblower
\cppfile{code_examples/sycl/buffer4.cpp}
\end{codebox}
         
\subsection{Building code}

Compiling code with SYCL should require passing ``-fsycl'' to a compiler that supports the SYCL 2020 specification, 
but it often turns out to be substantially more complicated than with standard C++23 or OpenMP.

There are many implementations of the SYCL specification (incl. OpenSYCL, Intel DPC++, CodePlay ComputeCPP)
with minor differences. 
One way of getting around the complexity is to install the OpenSYCL compiler:
\url{https://github.com/OpenSYCL/OpenSYCL} (formerly known as hipSYCL).  

Independent of the compiler, to build code with SYCL that targets a GPGPU, you need to link with the appropriate libraries, which depends on what is your target. 
If you target, e.g., common OpenCL variants: 


\raggedbottom
\begin{codebox}[]{}
\footnotesize Linking SYCL against OpenCL.
\tcblower
\cppfile{code_examples/sycl/link1.cpp}
\end{codebox}

If you target MKL:

\raggedbottom
\begin{codebox}[]{}
\footnotesize Linking SYCL against MKL.
\tcblower
\cppfile{code_examples/sycl/link2.cpp}
\end{codebox}

If you target CUDA (or NVIDIA OpenCL), you may need to set up paths to CUDA, in addition to the use of:

\begin{verbatim}
-fsycl -fsycl-targets=nvptx64-nvidia-cuda
\end{verbatim}

Similarly for AMD:

\begin{verbatim}
-fsycl -fsycl-targets=amdgcn-amd-amdhsa -Xsycl-target-backend â€“offload-arch=gfx906
\end{verbatim}

For details of the use of SYCL with CUDA, see \url{https://github.com/codeplaysoftware/SYCL-For-CUDA-Examples}
